{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports and global variable setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import xml.etree.ElementTree as ET\n",
    "import utils\n",
    "import re\n",
    "from utils import *\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from translate import translator\n",
    "from langdetect import detect\n",
    "from random import randrange\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import tqdm\n",
    "import dill\n",
    "from numbers import Number\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "from IPython.display import SVG\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# initializing Mongo DB Client for infomedia articles\n"
     ]
    }
   ],
   "source": [
    "## db variables\n",
    "host = \"mongodb://localhost:27017\"\n",
    "uid = \"\"\n",
    "pwd = \"\"\n",
    "client = None\n",
    "db = None\n",
    "sentiment_collection = None\n",
    "\n",
    "# connect to mongo\n",
    "#\n",
    "print(\"############# initializing Mongo DB Client for infomedia articles\")\n",
    "mclient = MongoClient(host)\n",
    "mdb = mclient.sentiments_db\n",
    "infomedia_collection = mdb.infomedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/sentiments/data /home/ubuntu/nbs/sentiments/data/original /home/ubuntu/nbs/sentiments/data/train\n",
      "model-ln-da-s40000-v2017-07-26.h5\n"
     ]
    }
   ],
   "source": [
    "## Global scope variables\n",
    "GLOBAL_WORDS_LIST = []\n",
    "WORD_TO_IDX = {}\n",
    "IDX_TO_WORD = {}\n",
    "CURRENT_LANGUAGE = \"da\"\n",
    "current_dir = os.getcwd()\n",
    "missing_words_count = 0\n",
    "missing_most_common_words = []\n",
    "DATA_DIR = current_dir + \"/sentiments/data\"\n",
    "DANISH_LN_WORDS = DATA_DIR+\"/danish_words.csv\"\n",
    "TEST_DATA_FILE_CSV = DATA_DIR+\"/batch2.csv\"\n",
    "ORIGINAL_DATA = DATA_DIR + \"/original\"\n",
    "TRAIN_DATA_DIR = DATA_DIR + \"/train\"\n",
    "TEST_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_tests_data.p\"\n",
    "TEST_LABEL_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_tests_labels.p\"\n",
    "ORIGINAL_TESTS_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_test_results_data.p\"\n",
    "ORIGINAL_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_results_data.p\"\n",
    "TRAIN_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_train_data.p\"\n",
    "TRAIN_LABEL_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_train_labels.p\"\n",
    "VALIDATE_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_validate_data.p\"\n",
    "VALIDATE_LABEL_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_validate_labels.p\"\n",
    "MODEL_PATH = current_dir + \"/sentiments/models/\"\n",
    "GLOBAL_LIST_DATA_FILE = DATA_DIR + \"/\"+CURRENT_LANGUAGE+\"_global_list.p\"\n",
    "WORD_TO_IDX_DATA_FILE = DATA_DIR + \"/\"+CURRENT_LANGUAGE+\"_words_to_idx.p\"\n",
    "IDX_TO_WORD_DATA_FILE = DATA_DIR + \"/\"+CURRENT_LANGUAGE+\"_idx_to_words.p\"\n",
    "MISSING_WORDS_DATA_FILE = DATA_DIR + \"/\"+CURRENT_LANGUAGE+\"_missing_words.p\"\n",
    "print DATA_DIR, ORIGINAL_DATA, TRAIN_DATA_DIR\n",
    "# maximum num of words the sentiment can process\n",
    "MAX_WORD_COUNT = 500\n",
    "vocabsize = 40000\n",
    "vocabsize_limit = 40000\n",
    "RESERVED_PROCESS_TERM_IDX = vocabsize\n",
    "DB_RECORD_EXTRACTION_LIMIT = 40000\n",
    "MODEL_NAME = \"model-ln-\"+CURRENT_LANGUAGE+\"-s\"+str(vocabsize)+\"-v2017-07-26\"+\".h5\"\n",
    "print MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for data setup:\n",
    "1. Read the articles with process terms and its sentiments.\n",
    "     1.a) the output will be an array of dict items\n",
    "          [{'id':'4s23s32', 'process_terms':['bestbuy, best buy'], text:'bestbuy is better than staples', 'sentiment':0.1},\n",
    "           {'id':'4s23s32', 'process_terms':['staples'], text:'bestbuy is better than staples', 'sentiment':1}]\n",
    "     \n",
    "2. enumerate the process terms and split it into different records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data_from_db(start=0, limit_size=10):\n",
    "    results = []\n",
    "    total = mdb.infomedia.find({}).count()\n",
    "    other_ln_count = 0\n",
    "    print(\"******** Total records in db = \"+str(total))\n",
    "    cursor = mdb.infomedia.find({}).skip(0).limit(DB_RECORD_EXTRACTION_LIMIT)\n",
    "#     f = FloatProgress(min=0, max=total)\n",
    "#     display(f)\n",
    "    for doc in cursor:\n",
    "        # print(doc)\n",
    "#         f.value += 1\n",
    "        if not doc.get('BodyText') or doc.get('BodyText').strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        r = {'id': str(doc.get('ArticleId')),\n",
    "             'text': doc.get('BodyText').lower().strip(),\n",
    "             'process_term': [],\n",
    "             'sentiment':0.0\n",
    "             }\n",
    "#         print(r)\n",
    "        source_ln = \"da\"\n",
    "#         try:\n",
    "#             source_ln = detect(r['text'])\n",
    "#         except:\n",
    "#              print(\"Failed to detect lang for text = \"+r['text'][:30])\n",
    "            \n",
    "        if source_ln == CURRENT_LANGUAGE:\n",
    "            process_terms= doc.get('kw')\n",
    "            scores = doc.get('score')\n",
    "            if process_terms != None:\n",
    "                for index,item in enumerate(process_terms):\n",
    "                    rcopy = r.copy()\n",
    "                    for x in item:\n",
    "                        pitem = x.encode(\"utf-8\").lower()\n",
    "                        rcopy['process_term'].extend([y for y in pitem.split(',')])\n",
    "                    rcopy['process_term'] = set(rcopy['process_term'])   \n",
    "                    rcopy['sentiment'] = scores[index]\n",
    "                    results.append(rcopy)\n",
    "        else:\n",
    "            other_ln_count += 1\n",
    "        \n",
    "    \n",
    "#     print(\"found db entries = \"+results.length())\n",
    "#     print results[:10]\n",
    "    print(\"******** Total records extracted from db = \"+str(len(results)))\n",
    "    print(\"***** saving results started...please wait.....\")    \n",
    "    dill.dump(results, open(ORIGINAL_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** completed saving results \")    \n",
    "    \n",
    "\n",
    "#load_training_data_from_db();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_from_file():\n",
    "    results = []\n",
    "    \n",
    "    with open(TEST_DATA_FILE_CSV, 'r') as f:\n",
    "        csvfile = csv.reader(f)\n",
    "        skip_header = True\n",
    "        pro = FloatProgress(min=0, max=50000)\n",
    "        display(pro)\n",
    "        pro.value = 0\n",
    "        for row in csvfile:\n",
    "            pro.value +=1\n",
    "            if skip_header:\n",
    "                skip_header = False\n",
    "                continue\n",
    "            r = {'process_term':[row[0]],\n",
    "                          'sentiment':float(row[1]),\n",
    "                          'text':row[3]\n",
    "                        }\n",
    "            results.append(r)\n",
    "#             process_terms= r['process_term']\n",
    "            \n",
    "#             if process_terms != None:\n",
    "#                 for index,item in enumerate(process_terms):\n",
    "#                     rcopy = r.copy()\n",
    "#                     for x in item:\n",
    "#                         pitem = x.encode(\"utf-8\").lower()\n",
    "#                         rcopy['process_term'].extend([y for y in pitem.split(',')])\n",
    "#                     rcopy['process_term'] = set(rcopy['process_term'])   \n",
    "#                     results.append(rcopy)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    print(\"saving the test data...recs = \"+str(len(results)))\n",
    "    dill.dump(results, open(ORIGINAL_TESTS_DATA_FILE, \"wb\" ))\n",
    "    print(\"finished!\")\n",
    "\n",
    "\n",
    "#load_test_data_from_file();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasNumbers(txt):\n",
    "    return bool(re.search(r'\\d', txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_idx(text, process_terms):\n",
    "    global WORD_TO_IDX\n",
    "    global vocabsize\n",
    "    global missing_words_count\n",
    "#     global missing_most_common_words\n",
    "    word_idx = []\n",
    "    p_terms = [x.lower().replace('*','') for x in process_terms]\n",
    "    text = text.lower().strip()\n",
    "    text = text.replace(\".\",\" . \")\n",
    "    text = text.replace(\",\",\" , \")\n",
    "  \n",
    "    for pt in p_terms:\n",
    "        try:\n",
    "            text = text.replace(pt, str(vocabsize + 1))\n",
    "        except UnicodeDecodeError:            \n",
    "            tx = text.encode('utf-8')\n",
    "            text = tx.replace(pt, str(vocabsize + 1))\n",
    "  \n",
    "\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "#         if word in p_terms:\n",
    "#             word_idx.append(vocabsize + 1)\n",
    "        if word == str(vocabsize + 1):\n",
    "            word_idx.append(vocabsize + 1)\n",
    "        else:\n",
    "            if (word not in WORD_TO_IDX):\n",
    "                if not hasNumbers(word):         \n",
    "                    word_idx.append(vocabsize)\n",
    "#                     if word not in missing_most_common_words:\n",
    "#                         missing_most_common_words.append(word)\n",
    "                    \n",
    "#                 print (\"word not found in vocab = \"+word)\n",
    "            else:\n",
    "                word_idx.append(WORD_TO_IDX[word])\n",
    "        \n",
    "    \n",
    "    return word_idx        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    GLOBAL_WORDS_LIST = set(pickle.load(open( GLOBAL_LIST_DATA_FILE, \"rb\" )))\n",
    "    clean_list = [w for w in GLOBAL_WORDS_LIST if not hasNumbers(w)]\n",
    "    print (\"list size before cleaning = \"+str(len(GLOBAL_WORDS_LIST)))\n",
    "    print (\"list size after cleaning = \"+str(len(clean_list)))\n",
    "    print(\"#### saving dictionary.... \")\n",
    "    dill.dump(clean_list, open(GLOBAL_LIST_DATA_FILE, \"wb\" ))\n",
    "    print(\"#### completed \")\n",
    "#clean_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_words_index():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    global WORD_TO_IDX\n",
    "    global IDX_TO_WORD\n",
    "    global vocabsize\n",
    "    global vocabsize_limit\n",
    "    \n",
    "    ## \n",
    "    # Build the word to idx and idx to word and dump it for later use\n",
    "    #\n",
    "    GLOBAL_WORDS_LIST = pickle.load(open( GLOBAL_LIST_DATA_FILE, \"rb\" ))\n",
    "    \n",
    "#     words_occurence = Counter(GLOBAL_WORDS_LIST)\n",
    "#     print(\"***** total unique words = \"+str(len(words_occurence)))\n",
    "\n",
    "#     GLOBAL_WORDS_LIST = [w for w,c in words_occurence.most_common(vocabsize_limit)]\n",
    "#     print (\"sorted dictionary word list = \")    \n",
    "    print(GLOBAL_WORDS_LIST[:20])        \n",
    "    vocabsize = min(vocabsize_limit,len(GLOBAL_WORDS_LIST))\n",
    "    GLOBAL_WORDS_LIST = GLOBAL_WORDS_LIST[:vocabsize]\n",
    "    \n",
    "    print(\"***** total words in dictionary = \"+str(len(GLOBAL_WORDS_LIST)))\n",
    "#     words_occurence = Counter(GLOBAL_WORDS_LIST)\n",
    "#     print(\"***** total unique words = \"+str(len(words_occurence)))\n",
    "#     sorted_words = [w for w,c in words_occurence.most_common(vocabsize-2)]\n",
    "#     print (\"sorted word = \")\n",
    "#     print(sorted_words[:10])\n",
    "#     missing_words = pickle.load(open(MISSING_WORDS_DATA_FILE, \"rb\" ))\n",
    "#     print(\"@@@ adding missing words\")\n",
    "#     GLOBAL_WORDS_LIST.append([w for w in missing_words if w not in GLOBAL_WORDS_LIST])\n",
    "    \n",
    "    WORD_TO_IDX = {w:idx for idx,w in enumerate(GLOBAL_WORDS_LIST)}\n",
    "    IDX_TO_WORD = {idx:w for idx,w in enumerate(GLOBAL_WORDS_LIST)}\n",
    "    assert len(WORD_TO_IDX) == len(IDX_TO_WORD)\n",
    "    print (\"word to idx size = \"+str(len(WORD_TO_IDX)))        \n",
    "    print(\"id to word size = \"+str(len(IDX_TO_WORD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    global vocabsize\n",
    "    GLOBAL_WORDS_LIST = []\n",
    "    vocabsize = 0\n",
    "    \n",
    "    #\n",
    "    # load from Danish dictionary words\n",
    "    #\n",
    "    print(\"about to open file = \"+DANISH_LN_WORDS)    \n",
    "    with open(DANISH_LN_WORDS, 'r') as f:\n",
    "        csvfile = csv.reader(f)        \n",
    "        for row in csvfile:\n",
    "            if row[0] not in GLOBAL_WORDS_LIST:\n",
    "                GLOBAL_WORDS_LIST.append(row[0])        \n",
    "        \n",
    "        f.close()\n",
    "    \n",
    "    ##\n",
    "    # Build a word list from all the training data\n",
    "    #    \n",
    "    results = pickle.load(open(ORIGINAL_DATA_FILE, \"rb\" ))\n",
    "    print(\"%%% about to build global word list from data files = \"+str(len(results)))    \n",
    "    f = FloatProgress(min=0, max=len(results))\n",
    "    display(f)        \n",
    "    for rec in results:\n",
    "        f.value += 1\n",
    "        text= rec['text'].lower()    \n",
    "        text = text.replace(\".\",\" . \")\n",
    "        text = text.replace(\",\",\" , \")\n",
    "        text = text.replace(\"'\",\" ' \")\n",
    "        text = text.replace(\"\\\"\",\" \\\" \")\n",
    "        words = text.split()\n",
    "        GLOBAL_WORDS_LIST.extend([x.strip() for x in words if not hasNumbers(x)]) \n",
    "#         GLOBAL_WORDS_LIST.extend([x for x in words])        \n",
    "    \n",
    "   \n",
    "        \n",
    "    if \",\" not in GLOBAL_WORDS_LIST:\n",
    "        GLOBAL_WORDS_LIST.append(\",\")\n",
    "    if \".\" not in GLOBAL_WORDS_LIST:\n",
    "        GLOBAL_WORDS_LIST.append(\".\")              \n",
    "    \n",
    "    \n",
    "    words_occurence = Counter(GLOBAL_WORDS_LIST)\n",
    "    print(\"***** total unique words = \"+str(len(words_occurence)))\n",
    "\n",
    "    GLOBAL_WORDS_LIST = [w for w,c in words_occurence.most_common()]\n",
    "    print (\"sorted dictionary word list = \")\n",
    "    print(GLOBAL_WORDS_LIST[:20])\n",
    "    \n",
    "    vocabsize = len(GLOBAL_WORDS_LIST)\n",
    "    print(\"#### total dictionary size = \"+str(len(GLOBAL_WORDS_LIST)))\n",
    "    print(\"#### saving dictionary.... \")\n",
    "    dill.dump(GLOBAL_WORDS_LIST, open(GLOBAL_LIST_DATA_FILE, \"wb\" ))\n",
    "    print(\"#### completed \")\n",
    "        \n",
    "#build_dictionary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def build_global_words_list():\n",
    "#     global GLOBAL_WORDS_LIST\n",
    "#     GLOBAL_WORDS_LIST = []\n",
    "#     print(\"%%% about to build global word list\")    \n",
    "#     f = FloatProgress(min=0, max=len(results))\n",
    "#     display(f)\n",
    "#     results = pickle.load(open(ORIGINAL_DATA_FILE, \"rb\" ))\n",
    "#     ##\n",
    "#     # Build a word list from all the training data\n",
    "#     #\n",
    "#     for rec in results:\n",
    "#         f.value += 1\n",
    "#         build_ln_word_idx_list(rec['text'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_training_data():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    global WORD_TO_IDX\n",
    "    global IDX_TO_WORD\n",
    "    global vocabsize\n",
    "    results = pickle.load(open(ORIGINAL_DATA_FILE, \"rb\" ))\n",
    "    test_results = pickle.load(open(ORIGINAL_TESTS_DATA_FILE, \"rb\" ))\n",
    "    \n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    validate_data = []\n",
    "    validate_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    train_process_terms = [[]]\n",
    "    word_list = []\n",
    "\n",
    "             \n",
    "    #\n",
    "    # load the words index\n",
    "    #        \n",
    "    load_words_index()\n",
    "    \n",
    "    #    \n",
    "    # convert the train_data words to indexes\n",
    "    #\n",
    "    print(\"*** converting texts to ids\")\n",
    "    f = FloatProgress(min=0, max=len(results))\n",
    "    display(f)\n",
    "    f.value = 0        \n",
    "#     orderedResults = sorted(results, key=itemgetter('sentiment'))         \n",
    "    for rec in results:\n",
    "        f.value += 1\n",
    "        src_data = (convert_words_to_idx(rec['text'].lower(),rec['process_term']))        \n",
    "        src_label = (rec['sentiment'])\n",
    "        \n",
    "        # save every third record as a validation record\n",
    "        if f.value % 3 == 0:\n",
    "            validate_data.append(src_data)        \n",
    "            validate_labels.append(src_label)             \n",
    "        \n",
    "        train_data.append(src_data)        \n",
    "        train_labels.append(src_label)\n",
    "            \n",
    "            \n",
    "    #    \n",
    "    # convert test results\n",
    "    #\n",
    "    print(\"*** converting test data texts to ids\")\n",
    "    f.value = 0        \n",
    "#     orderedTestResults = sorted(test_results, key=itemgetter('sentiment'))         \n",
    "    for rec in test_results:\n",
    "        f.value += 1\n",
    "        test_data.append(convert_words_to_idx(rec['text'].lower(),rec['process_term']))        \n",
    "        test_labels.append(rec['sentiment'])\n",
    "        \n",
    "\n",
    "#     GLOBAL_WORDS_LIST.append([w for w in missing_most_common_words if w not in GLOBAL_WORDS_LIST])\n",
    "    #\n",
    "    # pick random items from list for validation set\n",
    "    #\n",
    "#     validate_data = []\n",
    "#     validate_labels = []\n",
    "#     val_set_size = int(round((len(train_data) * 0.3), -1))\n",
    "#     print(\"validation set size = \"+str(val_set_size))\n",
    "#     for i in range(0,val_set_size):\n",
    "#         random_index = randrange(i,len(train_data))\n",
    "#         validate_data.append(train_data[random_index])\n",
    "#         validate_labels.append(train_labels[random_index])\n",
    "#         del train_data[random_index]\n",
    "#         del train_labels[random_index]\n",
    "       \n",
    "#     print(orderedResults[1])\n",
    "    print(train_data[1])\n",
    "    print(train_labels[1])\n",
    "    print(validate_data[1])\n",
    "    print(validate_labels[1])\n",
    "    print(\"*** vocabsize = \"+str(vocabsize))\n",
    "    print(\"validation data size = \"+str(len(validate_data)))\n",
    "    print(\"validation labels size = \"+str(len(validate_labels)))\n",
    "    print(\"train data size = \"+str(len(train_data)))\n",
    "    print(\"train labels size = \"+str(len(train_labels)))\n",
    "    print(\"===========================================================\")\n",
    "    print(test_data[1])\n",
    "    print(test_labels[1])\n",
    "    print(\"test data size = \"+str(len(test_data)))\n",
    "    print(\"test labels size = \"+str(len(test_labels)))\n",
    "    #\n",
    "    # save it for later use. \n",
    "    #\n",
    "        \n",
    "    print(\"***** saving files started...please wait.....\")       \n",
    "    dill.dump(train_data, open(TRAIN_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** saved train_data to pickle files..\")\n",
    "    dill.dump(train_labels, open(TRAIN_LABEL_FILE, \"wb\" ))\n",
    "    print(\"***** saved train_labels to pickle files..\")\n",
    "    dill.dump(validate_data, open(VALIDATE_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** saved validate_data to pickle files..\")\n",
    "    dill.dump(validate_labels, open(VALIDATE_LABEL_FILE, \"wb\" ))\n",
    "    print(\"***** saved validate_labels to pickle files..\")  \n",
    "    \n",
    "    print(\"***** saved test_data to pickle files..\")\n",
    "    dill.dump(test_data, open(TEST_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** saved test_data to pickle files..\")\n",
    "    dill.dump(test_labels, open(TEST_LABEL_FILE, \"wb\" ))\n",
    "    print(\"***** saved test_labels to pickle files..\")  \n",
    "    \n",
    "    print(\"***** SAVED ALL FILES.....\")\n",
    "   \n",
    "\n",
    "\n",
    "setup_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data_from_file():    \n",
    "    global WORD_TO_IDX\n",
    "    global IDX_TO_WORD\n",
    "    global vocabsize    \n",
    "    \n",
    "    WORD_TO_IDX = {}\n",
    "    IDX_TO_WORD = {}\n",
    "    \n",
    "    #\n",
    "    # load training data from file\n",
    "    #\n",
    "    train_data = pickle.load(open(TRAIN_DATA_FILE, \"rb\" ))\n",
    "    train_labels = pickle.load(open(TRAIN_LABEL_FILE, \"rb\" ))    \n",
    "    validate_data = pickle.load(open(VALIDATE_DATA_FILE, \"rb\" ))\n",
    "    validate_labels = pickle.load(open(VALIDATE_LABEL_FILE, \"rb\" ))\n",
    "    test_data = pickle.load(open(TEST_DATA_FILE, \"rb\" ))\n",
    "    test_labels = pickle.load(open(TEST_LABEL_FILE, \"rb\" ))\n",
    "     #\n",
    "    # load the words index\n",
    "    #\n",
    "    load_words_index()\n",
    "    \n",
    "    return train_data,train_labels, validate_data, validate_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "1. load the traing and validation data from disk\n",
    "2. Pad the data to match the shape of the Cnv\n",
    "3. Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_data(word_idx):        \n",
    "    return sequence.pad_sequences(word_idx,maxlen=MAX_WORD_COUNT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u',', u'.', u'i', 'og', 'at', 'er', 'det', 'en', 'til', u'p\\xe3\\xa5', 'for', 'af', u'\"', 'der', 'med', 'har', 'de', 'den', 'som', 'ikke']\n",
      "***** total words in dictionary = 40000\n",
      "word to idx size = 40000\n",
      "id to word size = 40000\n",
      "===================================================================================\n",
      "vocab size = 40000\n",
      "846\n",
      "konkurrence\n",
      "40000\n",
      "===================================================================================\n",
      "[[30309, 40000, 5, 20, 11, 16, 115, 1082, 14, 23677, 9, 2114, 3467, 0, 13, 9072, 11, 40000, 3, 2114, 14767, 40000, 9, 493, 1, 40000, 5, 20, 7370, 8, 16, 2704, 4890, 201, 0, 3, 16, 39384, 1034, 30, 729, 18, 20, 2350, 8, 17, 903, 0, 6278, 573, 9, 4971, 1, 27, 31, 67, 271, 10, 4, 634, 96, 9, 11603, 23, 455, 1, 0, 41, 16, 2, 217, 3861, 20869, 31, 40, 40000, 14, 4, 233, 277, 8, 7450, 3, 40000, 3295, 16, 31, 33, 74, 2089, 9, 7, 118, 11, 16, 198, 13058, 3517, 520, 3836, 612, 67, 1, 13, 31, 40, 7, 8, 65, 9, 7024, 665, 4624, 1, 3194, 9, 16, 1891, 31, 5300, 7, 18921, 3194, 14, 40000, 12, 40000, 214, 40000, 1, 16, 1891, 15, 66, 382, 7, 40000, 41, 16, 279, 1, 104, 1, 15, 4496, 18540, 0, 40000, 0, 3899, 2, 41, 174, 97, 9, 35795, 0, 3, 16, 15, 70, 5790, 40000, 3, 12943, 1, 74, 27, 880, 37, 4, 1509, 2, 6, 1891, 201, 9, 4971, 0, 30, 27, 8141, 37, 9, 1139, 1, 44, 9, 40000, 1, 40000, 1, 172, 834, 16386, 1]]\n",
      "-------------------------------\n",
      "[1]\n",
      "-------------------------------\n",
      "validation data size = 25368\n",
      "validation labels size = 25368\n",
      "train data size = 76104\n",
      "train labels size = 76104\n",
      "test data size = 55512\n",
      "test labels size = 55512\n",
      "Counter({1.0: 27627, 0.0: 20265, -1.0: 7620})\n"
     ]
    }
   ],
   "source": [
    "t_data, t_labels, v_data, v_labels, test_data, test_labels = load_training_data_from_file()\n",
    "print(\"===================================================================================\")\n",
    "print(\"vocab size = \"+str(vocabsize))\n",
    "print WORD_TO_IDX[u'konkurrence']\n",
    "print IDX_TO_WORD[WORD_TO_IDX[u'konkurrence']]\n",
    "print len(WORD_TO_IDX)\n",
    "index = randrange(0,len(t_data))\n",
    "print(\"===================================================================================\")\n",
    "print(t_data[index:index+1])\n",
    "print(\"-------------------------------\")\n",
    "print(t_labels[index:index+1])\n",
    "print(\"-------------------------------\")\n",
    "print(\"validation data size = \"+str(len(v_data)))\n",
    "print(\"validation labels size = \"+str(len(v_labels)))\n",
    "print(\"train data size = \"+str(len(t_data)))\n",
    "print(\"train labels size = \"+str(len(t_labels)))\n",
    "print(\"test data size = \"+str(len(test_data)))\n",
    "print(\"test labels size = \"+str(len(test_labels)))\n",
    "print Counter(test_labels)\n",
    "\n",
    "# print(pt[index:index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Counter(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Counter(t_labels)\n",
    "print Counter(v_labels)\n",
    "print IDX_TO_WORD[5001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76104\n",
      "25368\n",
      "55512\n",
      "(76104, 500)\n",
      "(25368, 500)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n",
      "[[0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0]]\n",
      "[[0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0]]\n",
      "40000\n",
      "500\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0 40000   114  1868\n",
      "  9097     0   114  1967 13901     0   114  1801     0   114  1756     5   157   105    59    11\n",
      "    16    60   687     0    13     5  1943    63     2 40000     1     3   127   217  5675  1967\n",
      "  2989     0   824  6688     0  3716  8969     3  1923  3840     1    19  7661    29    82   687\n",
      "     0    13  2997   159   168  1163    83    16   536  5360     1    26    13    73   136    33\n",
      "     7 40000     8   797     0     3    84  2710  1329     9     7   159 17917  1847     8   299\n",
      " 40000    19   350    91  1665 40000     1    63     5   135     6   190     0    22    15 40000\n",
      "    23   114 16209    23   730  1245   114  2497 40000     3     7   114  7211    14 40000     8\n",
      "     7  1492    23  7622     1  4168  5650  6649 40000    54    17   712 10470 40000  3916   122\n",
      " 40000   168    64  6649     0   196 19250     1  3916 10387   537 40000    17   115  1492    23\n",
      " 10387   537     1  3916 40000  5716  1655    11    17 26756     1  3916   730 40000    33   730\n",
      "  4159     7 16209     1  3916   114  2074    11    17  1145  2381     1  3916   114  2074    11\n",
      " 17232  1801     1  3916   159     7  1492    29   373   126     3     1  3916  1257    17   464\n",
      "    18 40000     1  3916    90   114  1655    11  4699     1  3916   114 40000  6105   123    14\n",
      "   824     1  3916   114  1970    11    17    82  4835     1  3916  1145   187  1492     0  1318\n",
      "     9     1  3916  4156 40000    17   289 11109     2   114  1655     1  3916  6416  6416 30193\n",
      "   114 10941  1492     1  3916   114 15063     2 40000     1  3916  1996 23603  1145  7886  1996\n",
      "    29     7   636 40000     1  3916  2497 40000   114  1655    11 32555  1801     1  3916  7622\n",
      " 40000 40000    82 40000     1  3916 40001 40000   114  2074    11  1774 16208     1  3916 40000\n",
      " 40000     0    13  5464   384     1   470     1  1257     1  3916  7211    17  5969     0     4\n",
      " 40000    19     5  1763     1  3916  1517  4254    48  5671    64     2 31811     1  3916 40000\n",
      "   873  1970    11    17  4835     1  3916  5226 40000    90   114  1655    11 40000     1  3916\n",
      "  1868 40000   114  1970    11 40000     1  3916 40000   159     7  7360    23  1868     0  4065\n",
      "  3317  5193     1  3916  1893 40000     7   289  4108     1  3916 40000   114  1655    14  2884\n",
      "    18     2   730     1  3916 18994 40000   244  1960  1655    11    17    55 18994     1  3916\n",
      "  4276 40000  7360     9   917     0 40000     1  3916   824 40000   114  7360  1183   123    14\n",
      "  1257     1  3916  1361 18575   114  1655    18  1492     1  3916  1801 30193    33  1801    18\n",
      "   424   360     1  3916     2     1   479     1  1066    85  7992     2 40000     1  3916  1756\n",
      "   137     3   114  1655    11 40000     2 19422     1  3916   114  1756     2     1  3916 40000\n",
      "    11     1  3916     1]\n"
     ]
    }
   ],
   "source": [
    "pad_train_data = get_padded_data(t_data)\n",
    "pad_validation_data = get_padded_data(v_data)\n",
    "pad_test_data = get_padded_data(test_data)\n",
    "print len(pad_train_data)\n",
    "print len(pad_validation_data)\n",
    "print len(pad_test_data)\n",
    "print pad_train_data.shape\n",
    "print pad_validation_data.shape\n",
    "v_bin_labels = [1 if x>=0 else 0 for x in v_labels]\n",
    "t_bin_labels = [1 if x>=0 else 0 for x in t_labels]\n",
    "\n",
    "# cat_one_hot_encode = {-1:[1,0,0,0,0],\n",
    "#                      -0.5:[0,1,0,0,0],\n",
    "#                      0:[0,0,1,0,0],\n",
    "#                      0.5:[0,0,0,1,0],\n",
    "#                      1:[0,0,0,0,1]}\n",
    "cat_one_hot_encode = {-1:[1,0,0],\n",
    "                     -0.5:[1,0,0],\n",
    "                     0:[0,1,0],\n",
    "                     0.5:[0,0,1],\n",
    "                     1:[0,0,1]}\n",
    "\n",
    "v_cat_labels = [cat_one_hot_encode[x] for x in v_labels]\n",
    "t_cat_labels = [cat_one_hot_encode[x] for x in t_labels]\n",
    "test_cat_labels = [cat_one_hot_encode[x] for x in test_labels]\n",
    "\n",
    "print v_bin_labels[:20]\n",
    "print t_bin_labels[:20]\n",
    "print v_cat_labels[:20]\n",
    "print t_cat_labels[:20]\n",
    "print test_cat_labels[:20]\n",
    "print vocabsize\n",
    "print MAX_WORD_COUNT\n",
    "print pad_train_data[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, -1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print test_labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabsize+2, output_dim=32, input_length=MAX_WORD_COUNT, dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, activation='relu', border_mode='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(),  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(pad_train_data,t_labels,validation_data=(pad_validation_data, v_labels), nb_epoch=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40001\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       1280064     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500, 32)       0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 500, 64)       6208        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 500, 64)       0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 250, 64)       0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           1600100     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 3)             303         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,886,675\n",
      "Trainable params: 2,886,675\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print vocabsize+1\n",
    "cat_model = Sequential()\n",
    "cat_model.add(Embedding(input_dim=vocabsize+2, output_dim=32, input_length=MAX_WORD_COUNT, dropout=0.2))\n",
    "cat_model.add(Dropout(0.3))\n",
    "cat_model.add(Convolution1D(64, 3, activation='relu', border_mode='same'))\n",
    "cat_model.add(Dropout(0.2))\n",
    "cat_model.add(MaxPooling1D())\n",
    "cat_model.add(Flatten())\n",
    "cat_model.add(Dense(100, activation='relu'))\n",
    "# cat_model.add(Dropout(0.1))\n",
    "cat_model.add(Dense(3, activation='softmax'))\n",
    "cat_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(),  metrics=['accuracy'])\n",
    "cat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(cat_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76104, 500)\n",
      "Train on 55512 samples, validate on 76104 samples\n",
      "Epoch 1/5\n",
      "55512/55512 [==============================] - 38s - loss: 0.5950 - acc: 0.7458 - val_loss: 0.4264 - val_acc: 0.8209\n",
      "Epoch 2/5\n",
      "55512/55512 [==============================] - 40s - loss: 0.4302 - acc: 0.8241 - val_loss: 0.4791 - val_acc: 0.7891\n",
      "Epoch 3/5\n",
      "55512/55512 [==============================] - 38s - loss: 0.3533 - acc: 0.8577 - val_loss: 0.4942 - val_acc: 0.7911\n",
      "Epoch 4/5\n",
      "55512/55512 [==============================] - 38s - loss: 0.3005 - acc: 0.8822 - val_loss: 0.5786 - val_acc: 0.7660\n",
      "Epoch 5/5\n",
      "55512/55512 [==============================] - 40s - loss: 0.2597 - acc: 0.8972 - val_loss: 0.4832 - val_acc: 0.8030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0c8a9cbd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print pad_train_data.shape\n",
    "# print [x for x in pad_validation_data if max(x) > vocabsize+1]\n",
    "#cat_model.fit(pad_train_data,t_cat_labels,validation_data=(pad_validation_data, v_cat_labels), nb_epoch=10, batch_size=64)\n",
    "# cat_model.fit(pad_train_data,t_cat_labels,validation_data=(pad_test_data, test_cat_labels), nb_epoch=20, batch_size=64)\n",
    "cat_model.fit(pad_test_data,test_cat_labels,validation_data=(pad_train_data, t_cat_labels), nb_epoch=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#binary model\n",
    "model.save_weights(MODEL_PATH + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#categorical modeling\n",
    "cat_model.save_weights(MODEL_PATH + \"ty_cat_\"+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test / Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_PATH + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model.load_weights(MODEL_PATH + \"ty_cat_\"+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text=\"The board network Board Network, which mentions itself as \\\"Denmark\\'s most exclusive board network\\\", has presented a financial statement for 2016, which is filled with errors and remarks by the auditor.Board Network is a company that makes networking for board members and organizes several events for its members. A membership is paid in NOK 10,000 per year before VAT.Board Networks advisory committee has known members such as Jens Moberg, Chairman of the Board of PostNord and Grundfos, and Eivind Kolding, former director of Danske Bank Prominent management center is in deep crisis. But the accounts of the celebre club get a lot of hug from the auditor, who does not make a conclusion on the accounts at all.Firstly, equity is negative, and the auditor has not seen an acceptable plan for how to change it.In addition, there are a series of legitimate offenses that may end with punishment to the owner.'The company is in violation of the Companies Act section 210, paragraph 1. 1, granted a loan to the company\\'s capital owner, whereby management can incur liability. ''In violation of the Withholding Tax Act, the company has not contained and reported taxes thereon, whereby the management may incur liability.''The company has in breach of the VAT Act reported incorrect VAT returns to Tax, which may lead to liability.'\\\"In violation of the Danish Financial Statements Act, the company has not prepared the annual report in a timely manner, whereby the management can incur liability, the auditor strikes.\\\"\\\"We can never be happy to make an account with remarks.\\\"He emphasizes that there is no risk of the bankruptcy of the company.According to the financial statements, the company has granted a loan to Jacob Stengel, which is in violation of the Companies Act.\"\n",
    "# text = \"Hasbro-holdet leverede endnu et stærkt kvartal på tværs af vores brands.\"\n",
    "# text = \"which mentions itself as \\\"Denmark\\'s most exclusive board\"\n",
    "#text = \"Bestyrelsesnetværket Board Network, der nævner sig som \\\"Danmarks mest eksklusive bestyrelsesnetværk \\\", har aflagt en finansieringsoversigt for 2016, som er fyldt med fejl og bemærkninger fra revisor. Board Network er et firma, der gør netværk Til bestyrelsesmedlemmer og organiserer flere arrangementer for sine medlemmer. Et medlemskab er betalt i kr. 10.000 pr. År før moms. Børnetværkets rådgivende udvalg har kendt medlemmer som Jens Moberg, bestyrelsesformand for PostNord og Grundfos, og Eivind Kolding, tidligere direktør for Danske Bank Prominent Management Center er i dyb krise. Men celebre-klubbernes regnskaber får en masse kram fra revisoren, som ikke har en konklusion på regnskabet. For det første er egenkapitalen negativ, og revisoren har ikke set en acceptabel plan for, hvordan man ændrer det. Derudover er der en række lovlige lovovertrædelser, der kan ende med straf for ejeren. «Selskabet overtræder aktielovens § 210, stk. 1. 1, ydet et lån til selskabets kapitalindehaver, hvorved ledelsen Kan pådrage sig ansvar. '' I strid med skatteloven har selskabet ikke indeholdt og rapporteret skatter deraf, hvorved ledelsen kan pådrage sig ansvar. '' Selskabet har i strid med momsloven rapporteret ukorrekt momsafkast til skat, hvilket kan medføre ansvar . '\\ \"I strid med årsregnskabsloven har selskabet ikke udarbejdet årsrapporten rettidigt, hvorved ledelsen kan pådrage sig ansvar, revisor slår. \\\" \\ \"Vi kan aldrig være glade for at lave en konto Med bemærkninger. \"Han understreger, at der ikke er nogen risiko for selskabets konkurs. Ifølge årsregnskabet har selskabet ydet lån til Jacob Stengel, der er i strid med selskabsloven.\"\n",
    "#text = \"Bestyrelsesnetværket Board Network, der nævner sig som \\\"Danmarks mest eksklusive bestyrelsesnetværk \\\", har aflagt en finansieringsoversigt for 2016, som er fyldt med fejl og bemærkninger fra revisor.\"\n",
    "text = \"Bestyrelsesnetværket Board Network, For det første er egenkapitalen negativ, og revisor har ikke set en acceptabel plan for, hvordan man ændrer den. Derudover er der en række lovlige lovovertrædelser, der kan ende med straf til ejeren\"\n",
    "# text = \"19 personer er blevet syge af en type salmonella-bakterie, der spores tilbage til en færdigret med forloren hare fra Coop.\"\n",
    "processed_terms = [\n",
    "        \"Board Network\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_keys = [\"-1   (Very Negative)\",\"-0.5 (Negative)\",\"0    (Neutral)\" ,\"0.5  (Positive)\",\"1    (Very Positive)\"]\n",
    "\n",
    "def predict(text, process_terms):\n",
    "    \n",
    "    global category_keys\n",
    "    \n",
    "    textWordsIdxs = convert_words_to_idx(text,processed_terms)\n",
    "    textIdxArrayPadded = get_padded_data([np.array(textWordsIdxs)])\n",
    "    \n",
    "    print cat_model.predict(textIdxArrayPadded, batch_size=1,verbose=1)\n",
    "    \n",
    "    print textIdxArrayPadded\n",
    "#     preds = dict(zip(category_keys, cat_model.predict(textIdxArrayPadded, batch_size=1,verbose=1)[0]))\n",
    "#     ordered_preds_keys = sorted(preds,key=preds.get, reverse=True)    \n",
    "#     num = float(((ordered_preds_keys[0])[:5]).strip())\n",
    "    \n",
    "#     return num\n",
    "#     print (\"\\n\\n------------------- model prediction (probabilities) -------------------\")\n",
    "#     print(\"\\n\".join([x.ljust(20)+\" \\t\\t \"+str(preds[x]) for x in ordered_preds_keys]))\n",
    "    # print(\"\\n\".join(ordered_preds_keys))\n",
    "#     print(\"\\n\\n====================R E S U L T=====================================\")`\n",
    "#     print \"sentiment = \"+ordered_preds_keys[0]+\" = \"+str(preds[ordered_preds_keys[0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[ 0.0089  0.9671  0.024 ]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0 40000 40001     0    10\n",
      "      6 40000     5  6052  2103     0     3  3465    15    19   179     7 21259   998    10     0\n",
      "    144    27 40000    17     1   673     5    13     7 40000 12501 40000     0    13    24   653\n",
      "     14  4155     8  4424]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print predict(text,processed_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0     0     0     0   584\n",
      "      5   116  2036    11     7  1258 16516     0    13 21149   176     8     7 40000    14 11048\n",
      "   6406    23   648     1]]\n",
      "1/1 [==============================] - 0s\n",
      "\n",
      "\n",
      "------------------- model prediction (probabilities) -------------------\n",
      "-1   (Very Negative) \t\t 0.96975\n",
      "-0.5 (Negative)      \t\t 0.0291242\n",
      "0    (Neutral)       \t\t 0.00112563\n",
      "\n",
      "\n",
      "====================R E S U L T=====================================\n",
      "sentiment = -1   (Very Negative) = 0.96975\n"
     ]
    }
   ],
   "source": [
    "#text_clean = re.sub('\\W+', ' ', text)\n",
    "print len(text.split())\n",
    "category_keys = [\"-1   (Very Negative)\",\"-0.5 (Negative)\",\"0    (Neutral)\" ,\"0.5  (Positive)\",\"1    (Very Positive)\"]\n",
    "textWordsIdxs = convert_words_to_idx(text,processed_terms)\n",
    "textIdxArrayPadded = get_padded_data([np.array(textWordsIdxs)])\n",
    "print textIdxArrayPadded\n",
    "preds = dict(zip(category_keys, cat_model.predict(textIdxArrayPadded, batch_size=1,verbose=1)[0]))\n",
    "ordered_preds_keys = sorted(preds,key=preds.get, reverse=True)\n",
    "print (\"\\n\\n------------------- model prediction (probabilities) -------------------\")\n",
    "print(\"\\n\".join([x.ljust(20)+\" \\t\\t \"+str(preds[x]) for x in ordered_preds_keys]))\n",
    "# print(\"\\n\".join(ordered_preds_keys))\n",
    "print(\"\\n\\n====================R E S U L T=====================================\")\n",
    "print \"sentiment = \"+ordered_preds_keys[0]+\" = \"+str(preds[ordered_preds_keys[0]])\n",
    "# print ordered_preds\n",
    "# print \"probability difference = \"+str(preds[ordered_preds[0]] - preds[ordered_preds[1]])\n",
    "# if (preds[ordered_preds[0]] - preds[ordered_preds[1]]) > 0.10:\n",
    "\n",
    "# else:\n",
    "#     print \"sentiment = \"+str(ordered_preds[1])\n",
    "# for i,x in enumerate(prediction[0]):\n",
    "#     if x == max(prediction[0]):\n",
    "#         print \"sentiment = \"+str(categories[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55512/55512 [==============================] - 12s    \n",
      "acc: 95.35%\n",
      "[0.16878117737551557, 0.95350554832413692]\n",
      "75904/76104 [============================>.] - ETA: 0sacc: 80.30%\n",
      "[0.48318916401326262, 0.8030195521792064]\n"
     ]
    }
   ],
   "source": [
    "# cat_model.test_on_batch(pad_test_data[:20000],test_cat_labels[:20000])\n",
    "# scores = cat_model.evaluate(pad_test_data, test_cat_labels, batch_size=128)\n",
    "scores = cat_model.evaluate(pad_test_data, test_cat_labels, batch_size=128)\n",
    "print(\"%s: %.2f%%\" % (cat_model.metrics_names[1], scores[1]*100))\n",
    "print scores\n",
    "\n",
    "scores = cat_model.evaluate(pad_train_data, t_cat_labels, batch_size=128)\n",
    "print(\"%s: %.2f%%\" % (cat_model.metrics_names[1], scores[1]*100))\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "0ab4b92ef74b487c874af94d664aa1d2": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
