{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using CNN\n",
    "\n",
    "2. Load training dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xml.etree.ElementTree as ET\n",
    "import utils\n",
    "from utils import *\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/sentiments/data /home/ubuntu/nbs/sentiments/data/original /home/ubuntu/nbs/sentiments/data/train\n",
      "model-s50000-v06-06-28.h5\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "DATA_DIR = current_dir + \"/sentiments/data\"\n",
    "ORIGINAL_DATA = DATA_DIR + \"/original\"\n",
    "TRAIN_DATA = DATA_DIR + \"/train\"\n",
    "NEG_REVIEWS_TRAIN_FILE = TRAIN_DATA + \"/negetivereviews.txt\"\n",
    "POS_REVIEWS_TRAIN_FILE = TRAIN_DATA + \"/positivereviews.txt\"\n",
    "MODEL_PATH = current_dir + \"/sentiments/models/\"\n",
    "WORDS_TO_IDX_DATA_FILE = DATA_DIR + \"/words_to_idx.p\"\n",
    "IDX_TO_WORD_DATA_FILE = DATA_DIR + \"/idx_to_words.p\"\n",
    "print DATA_DIR, ORIGINAL_DATA, TRAIN_DATA\n",
    "STR_MAX_LEN = 500\n",
    "vocabsize = 50000\n",
    "MODEL_NAME = \"model-s\"+str(vocabsize)+\"-v06-06-28\"+\".h5\"\n",
    "print MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - One time \n",
    "\n",
    "review file format\n",
    "\n",
    "      <review>\n",
    "            <unique_id>B00005UKBG:bad:j._brodeur_\"disgusted_consumer\"</unique_id>\n",
    "            <asin>B00005UKBG</asin>\n",
    "            <product_name>Atlantic 1316 CD Storage Case (110-Capacity, Wave): Electronics</product_name>\n",
    "            <product_type>electronics</product_type>\n",
    "            <helpful>15 of 16</helpful>\n",
    "            <rating>2.0</rating>\n",
    "            <title>bad</title>\n",
    "            <date>May 4, 2005</date>\n",
    "            <reviewer>J. Brodeur \"disgusted consumer\"</reviewer>\n",
    "            <reviewer_location>\t</reviewer_location>\n",
    "            <review_text>cons tips extremely easy on carpet and if you have a lot of cds stacked at the\ttop</review_text>\n",
    "       </review>\n",
    "\n",
    "1. Prepare training dataset from reviews folder\n",
    "        a.Read the positive reviews xml and copy the first top 1000 into an array\n",
    "        b.Read the negetive reviews xml and copy the first top 1000 into the array\n",
    "        c.Save the array into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read negetive reviews\n",
    "tree = ET.parse(ORIGINAL_DATA+\"/negative.review.xml\")\n",
    "root = tree.getroot()\n",
    "negetiveTexts = [txt.text for txt in root.iter('review_text')]\n",
    "\n",
    "#Open file \n",
    "negetiveReviewsFile = open(NEG_REVIEWS_TRAIN_FILE,\"w+\")\n",
    "negetiveReviewsFile.write(\"\\n\".join(negetiveTexts[:1000]).encode('utf-8').strip())\n",
    "\n",
    "# read positive reviews\n",
    "#Open file \n",
    "print \"reading file = \"+ ORIGINAL_DATA+\"/positive.review.xml\"\n",
    "tree = ET.parse(ORIGINAL_DATA+\"/positive.review.xml\")\n",
    "root = tree.getroot()\n",
    "positiveTexts = [txt.text for txt in root.iter('review_text')]\n",
    "\n",
    "#writing to file \n",
    "print \"writing  file = \"+ POS_REVIEWS_TRAIN_FILE\n",
    "positiveReviewsFile = open(POS_REVIEWS_TRAIN_FILE,\"w+\")\n",
    "positiveReviewsFile.write(\"\\n\".join(positiveTexts[:1000]).encode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "1. Load imdb reviews words list from keras imdb db\n",
    "2. Prepare an idx to word map.\n",
    "3. sort the words to get top most used words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load this one time and dump into a file for reuse\n",
    "imdbwords = imdb.get_word_index();\n",
    "pickle.dump(imdbwords, open(WORDS_TO_IDX_DATA_FILE, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34701\n",
      "fawn\n",
      "88584\n"
     ]
    }
   ],
   "source": [
    "widx = pickle.load(open( WORDS_TO_IDX_DATA_FILE, \"rb\" ))\n",
    "idx2word = {v:k for k,v in widx.iteritems()}\n",
    "print widx[\"fawn\"]\n",
    "print idx2word[34701]\n",
    "print len(widx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word index so that we can get the top 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'are', 'his', 'have', 'he', 'be', 'one', 'all', 'at', 'by', 'an', 'they', 'who', 'so', 'from', 'like', 'her', 'or', 'just', 'about', \"it's\", 'out', 'has', 'if', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'up', 'no', 'time', 'she', 'even', 'my', 'would', 'which', 'only', 'story', 'really', 'see', 'their', 'had', 'can', 'were', 'me', 'well', 'than', 'we', 'much', 'been', 'bad', 'get', 'will', 'do', 'also', 'into', 'people', 'other', 'first', 'great', 'because', 'how', 'him', 'most', \"don't\", 'made', 'its', 'then', 'way', 'make', 'them', 'too', 'could', 'any', 'movies', 'after']\n"
     ]
    }
   ],
   "source": [
    "widx_sorted_list = sorted(widx, key=widx.get)\n",
    "print widx_sorted_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "1. download the english training dataset and labels from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "file = open(path, 'rb')\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:1])\n",
    "print(train_data[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\" \".join(idx2word[v] for v in train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trnData = [np.array([i if i < vocabsize -1 else vocabsize -1 for i in s]) for s in train_data]\n",
    "testData = [np.array([i if i < vocabsize -1 else vocabsize -1 for i in s]) for s in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23022   309     6     3  1069   209     9  2175    30     1   169    55    14    46    82  5869\n",
      "    41   393   110   138    14  5359    58  4477   150     8     1  5032  5948   482    69     5\n",
      "   261    12 23022 49999  2003     6    73  2436     5   632    71     6  5359     1 25279     5\n",
      "  2004 10471     1  5941  1534    34    67    64   205   140    65  1232 49999 21145     1 49265\n",
      "     4     1   223   901    29  3024    69     4     1  5863    10   694     2    65  1534    51\n",
      "    10   216     1   387     8    60     3  1472  3724   802     5  3521   177     1   393    10\n",
      "  1238 14030    30   309     3   353   344  2989   143   130     5  7804    28     4   126  5359\n",
      "  1472  2375     5 23022   309    10   532    12   108  1470     4    58   556   101    12 23022\n",
      "   309     6   227  4187    48     3  2237    12     9   215]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print trnData[0]\n",
    "print train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lenstats = np.array(map(len, trnData))\n",
    "lenstats.max(), lenstats.min(), lenstats.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To make a perfect matrix , pad the array with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0 23022   309     6     3  1069   209\n",
      "     9  2175    30     1   169    55    14    46    82  5869    41   393   110   138    14  5359\n",
      "    58  4477   150     8     1  5032  5948   482    69     5   261    12 23022 49999  2003     6\n",
      "    73  2436     5   632    71     6  5359     1 25279     5  2004 10471     1  5941  1534    34\n",
      "    67    64   205   140    65  1232 49999 21145     1 49265     4     1   223   901    29  3024\n",
      "    69     4     1  5863    10   694     2    65  1534    51    10   216     1   387     8    60\n",
      "     3  1472  3724   802     5  3521   177     1   393    10  1238 14030    30   309     3   353\n",
      "   344  2989   143   130     5  7804    28     4   126  5359  1472  2375     5 23022   309    10\n",
      "   532    12   108  1470     4    58   556   101    12 23022   309     6   227  4187    48     3\n",
      "  2237    12     9   215]\n",
      "(25000, 500)\n",
      "500\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "padTrnData = sequence.pad_sequences(trnData, maxlen=STR_MAX_LEN, value=0)\n",
    "padTestData = sequence.pad_sequences(testData, maxlen=STR_MAX_LEN, value=0)\n",
    "print padTrnData[0]\n",
    "print padTestData.shape\n",
    "print STR_MAX_LEN\n",
    "print vocabsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the CNN\n",
    "1. build the model \n",
    "2. Train the model \n",
    "3. save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       1600000     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500, 32)       0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 500, 64)       10304       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 500, 64)       0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 250, 64)       0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           1600100     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3210505\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabsize, output_dim=32, input_length=STR_MAX_LEN, dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, activation='relu', border_mode='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(),  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(padTrnData,train_labels,validation_data=(padTestData, test_labels), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(MODEL_PATH + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "1. take a text \n",
    "2. split it into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = \"dont know what could have saved limp dispiriting yam but it definitely wasnt a lukewarm mushroom as murky and appealing as bong water\"\n",
    "#text = \"Oh and to top it all off the staff was taking photo in front of the food so we couldn't get in and the manager was five feet away not even paying attention \"\n",
    "#text = \"When I made the reservation they made sure to ask when and where our show was to ensure we had enough time to dine and still make it to our show\"\n",
    "#text = \"Very nice little place. Prices are reasonable and food was good. Staff is courteous and patient with a non Spanish speaking guy...lol.\"\n",
    "#text = \"food was very good , but the service was amazing\"\n",
    "text=\"the director in that company is not good\"\n",
    "\n",
    "#text=\"I ordered 2 lb bag of the Canada Green grass seed. It germinated but then died within three weeks. They said that there was an unconditional money back or replacement guarantee on the product. I have contacted the company through their Best Buy Direct distributor and was told that someone front he... company would be contacting me. It has been here weeks and still nothing. I would like to file a complaint for the lack of response and the replacement refund guarantee.\"\n",
    "\n",
    "#text = \"The parents of two of the seven cousins killed in that crash have sued the truck's driver and the trucking company\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_PATH + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text_clean = re.sub('\\W+', ' ', text)\n",
    "textWordsArray = np.array(text.lower().split())\n",
    "print len(textWordsArray)\n",
    "#textWordsArray\n",
    "textWordsIdx = []\n",
    "#textWordsIdxArray = [np.array(map(widx[i],(i if vocabsize -1 > i else vocabsize-1 for i in textWordsArray)))]\n",
    "\n",
    "for w in textWordsArray:\n",
    "    word = ''.join(c for c in w if (c.isalnum() or c == \"'\"))\n",
    "    if word not in widx:\n",
    "        textWordsIdx.append(vocabsize - 1)\n",
    "        print \" not found word = \"+word\n",
    "    elif widx[word] > vocabsize -1:\n",
    "        textWordsIdx.append(vocabsize - 1)\n",
    "        print \"rare word = \"+ word\n",
    "    else:\n",
    "        textWordsIdx.append(widx[word])\n",
    "\n",
    "textWordsIdxArray = [np.array(textWordsIdx)]\n",
    "#textWordsIdxArray = [np.array([i if i < vocabsize -1 else vocabsize -1 for i in s]) for s in textWordsIdxArray]\n",
    "#print textWordsIdx\n",
    "\n",
    "#print textWordsIdxArray\n",
    "textIdxArrayPadded = sequence.pad_sequences(textWordsIdxArray,maxlen=STR_MAX_LEN, value=0)\n",
    "#print textIdxArrayPadded\n",
    "prediction = model.predict(textIdxArrayPadded, batch_size=1,verbose=1)\n",
    "print prediction\n",
    "if prediction[0][0] > 0.60:\n",
    "    print \"POSITIVE\"\n",
    "elif prediction[0][0] < 0.50:\n",
    "    print \"NEGETIVE\"\n",
    "else:\n",
    "    print \"NEUTRAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
